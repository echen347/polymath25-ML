import numpy as np
import torch
import torch.nn as nn
import matplotlib.pyplot as plt


# normalizing flows

#start with a base distribution 1d
base_dist = torch.distributions.normal.Normal(0,1)

#target distribution
target = torch.distributions.Normal(2.0, 0.5)


# start with the transformation for a simple case assume T(x) = a + bx

class Transformation(nn.Module):
  def __init__(self):
    super().__init__()
    self.a = nn.Parameter(torch.randn(1))
    self.b = nn.Parameter(torch.randn(1))

  def forward(self, x):
    #return the value and log det of jacobian
    return self.a + self.b * x, torch.log(torch.abs(self.b))

  def inverse(self, x):
        z = (x - self.a) / self.b
        log_det = -torch.log(torch.abs(self.b))
        return z, log_det


class MultiTransformation(nn.Module):
    def __init__(self, n):
        super().__init__()
        self.flows = nn.ModuleList([Transformation() for _ in range(n)])

    def forward(self, x):
        log_det = torch.zeros_like(x)
        for flow in self.flows:
            x, ld = flow(x)
            log_det += ld
        return x, log_det

    def inverse(self, x):
        log_det = torch.zeros_like(x)
        for flow in reversed(self.flows):
            x, ld = flow.inverse(x)
            log_det += ld
        return x, log_det


class NormalizingFlows(nn.Module):
    def __init__(self, num_flows):
        super().__init__()
        self.flow_chain = MultiTransformation(num_flows)
        self.base_dist = torch.distributions.Normal(0., 1.)

    def log_prob(self, x):
        # Compute inverse pass to get z and total log-det
        z, log_det = self.flow_chain.inverse(x)
        log_pz = self.base_dist.log_prob(z)
        # Return mean log-likelihood over the batch
        return (log_pz + log_det).mean()

    def sample(self, num_samples):
        # Sample from base distribution and push through flows
        z0 = self.base_dist.sample((num_samples,))
        x, _ = self.flow_chain(z0)
        return x



target_dist = torch.distributions.Normal(2.0, 0.5)

model = NormalizingFlows(3)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
x_data = target_dist.sample((10000,))
for epoch in range(7000):
    optimizer.zero_grad()
    loss = -model.log_prob(x_data)
    loss.backward()
    optimizer.step()

    if not ((epoch+1)%500):
        print(f"Epoch {epoch+1}, Loss: {loss.item():.7f}")





import numpy as np
import torch

# Sample data from the target distribution
target_dist = model
sampled_target_data = target_dist.sample(10000).detach().numpy()

base_dist = torch.distributions.normal.Normal(2,0.5)
sampled_base_data = base_dist.sample((10000,)).numpy()


# Plot a histogram
plt.figure(figsize=(8, 4))
plt.hist(sampled_target_data, bins=50, density=True, alpha=0.6, color='g', label='Target Distribution')
plt.hist(sampled_base_data, bins=50, density=True, alpha=0.6, color='b', label='Base Distribution')
plt.title('Histogram of Sampled Data')
plt.xlabel('Value')
plt.ylabel('Frequency')
plt.legend()
plt.show()

plt.show()





class CouplingLayer(nn.Module):
    def __init__(self, mask, input_dim, hidden_dim, num_hidden):
        super().__init__()
        self.register_buffer('mask', mask)
        self.input_indices = (mask == 1).nonzero(as_tuple=True)[0]
        self.output_indices = (mask == 0).nonzero(as_tuple=True)[0]
        self.in_features = len(self.input_indices)
        self.out_features = len(self.output_indices)

        def build_net():
            layers = [nn.Linear(self.in_features, hidden_dim), nn.ReLU()]
            for _ in range(num_hidden):
                layers += [nn.Linear(hidden_dim, hidden_dim), nn.ReLU()]
            layers.append(nn.Linear(hidden_dim, self.out_features))
            return nn.Sequential(*layers)

        self.s_net = build_net()
        self.t_net = build_net()

    def forward(self, x):
        x1 = x[:, self.input_indices]
        x2 = x[:, self.output_indices]

        s = self.s_net(x1)
        t = self.t_net(x1)

        y2 = x2 * torch.exp(s) + t
        y = x.clone()
        y[:, self.output_indices] = y2

        log_det = torch.sum(s, dim=1)
        return y, log_det

    def inverse(self, y):
        y1 = y[:, self.input_indices]
        y2 = y[:, self.output_indices]

        s = self.s_net(y1)
        t = self.t_net(y1)

        x2 = (y2 - t) * torch.exp(-s)
        x = y.clone()
        x[:, self.output_indices] = x2

        log_det = -torch.sum(s, dim=1)
        return x, log_det



class RealNVP(nn.Module):
    def __init__(self, num_coupling_layers, input_dim, hidden_dim, num_hidden):
        super().__init__()
        self.layers = nn.ModuleList()
        self.input_dim = input_dim
        for i in range(num_coupling_layers):
            mask = self._create_mask(i)
            self.layers.append(CouplingLayer(mask, input_dim, hidden_dim, num_hidden))

    def _create_mask(self, i):
        mask = torch.zeros(self.input_dim)
        mask[i%2::2] = 1
        return mask

    def forward(self, x):
        log_det = torch.zeros(x.shape[0])
        for layer in self.layers:
            x, ld = layer(x)
            log_det += ld
        return x, log_det

    def inverse(self, z):
        log_det = torch.zeros(z.shape[0])
        for layer in reversed(self.layers):
            z, ld = layer.inverse(z)
            log_det += ld
        return z, log_det

    def log_prob(self, x):
        z, log_det = self.forward(x)
        log_pz = -0.5 * torch.sum(z**2, dim=1) - 0.5 * x.shape[1] * torch.log(torch.tensor(2 * torch.pi))
        return log_pz + log_det


# init the dataset using make_moons
from sklearn.datasets import make_moons, make_s_curve
X, _ = make_s_curve(n_samples=10_000, noise=0.1, random_state=42)


import mpl_toolkits.mplot3d  # noqa: F401
from matplotlib import ticker
def plot_3d(points, title):
    x, y, z = points.T

    fig, ax = plt.subplots(
        figsize=(6, 6),
        facecolor="white",
        tight_layout=True,
        subplot_kw={"projection": "3d"},
    )
    fig.suptitle(title, size=16)
    col = ax.scatter(x, y, z, s=50, alpha=0.8)
    ax.view_init(azim=-60, elev=9)
    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))
    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))
    ax.zaxis.set_major_locator(ticker.MultipleLocator(1))

    fig.colorbar(col, ax=ax, orientation="horizontal", shrink=0.6, aspect=60, pad=0.01)
    plt.show()



plot_3d(X, "Graph")


import matplotlib.pyplot as plt

plt.scatter(X[:,0], X[:,1])


from torch.utils.data import DataLoader, TensorDataset
from torch import optim

X = torch.tensor(X, dtype=torch.float32)
dataset = TensorDataset(X)
dataloader = DataLoader(dataset, batch_size=128, shuffle=True)


# training loop

model = RealNVP(6, 3, 128, 2)
optimizer = optim.Adam(model.parameters(), lr=1e-3)

n_epochs = 30

for epoch in range(n_epochs):
    total_loss = 0
    for batch in dataloader:
        x_batch = batch[0]
        optimizer.zero_grad()
        log_likelihood = model.log_prob(x_batch)
        loss = -log_likelihood.mean()  # maximize likelihood = minimize negative log-likelihood
        loss.backward()
        optimizer.step()
        total_loss += loss.item() * x_batch.size(0)

    avg_loss = total_loss / len(dataloader.dataset)
    print(f"Epoch {epoch+1}/{n_epochs}, Loss: {avg_loss:.4f}")



# sample from the model
def sample(model, num_samples):
    with torch.no_grad():
        z = torch.randn(num_samples, 3)
        x, _ = model.inverse(z)
    return x.numpy()



sampled_data = sample(model, 1000)

plot_3d(sampled_data, "Sampled Data")


# Sahith Edula
from sklearn.datasets import make_classification
from sklearn.decomposition import PCA
from torch.utils.data import DataLoader, TensorDataset
from torch import optim

# Data Generation
n_samples = 5000
n_features = 50 # Number of dimensions
n_classes = 4 
n_clusters_per_class = 3 # Number of sub-classes per class

X_np, y_np = make_classification(
    n_samples=n_samples,
    n_features=n_features,
    n_informative=15,
    n_redundant=5,
    n_classes=n_classes,
    n_clusters_per_class=n_clusters_per_class,
    flip_y=0.02,
    random_state=42
)

# Convert to PyTorch tensors and create DataLoader
X = torch.tensor(X_np, dtype=torch.float32)
dataset = TensorDataset(X)
dataloader = DataLoader(dataset, batch_size=128, shuffle=True)

print(f"Generated a dataset with shape: {X.shape}")

# Model Training
model = RealNVP(num_coupling_layers=8, input_dim=n_features, hidden_dim=256, num_hidden=2)
optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)
n_epochs = 50

print("\nStarting model training...")
for epoch in range(n_epochs):
    total_loss = 0
    for batch in dataloader:
        x_batch = batch[0]
        optimizer.zero_grad()
        log_likelihood = model.log_prob(x_batch)
        loss = -log_likelihood.mean()
        loss.backward()
        optimizer.step()
        total_loss += loss.item() * x_batch.size(0)

    avg_loss = total_loss / len(dataloader.dataset)
    if (epoch + 1) % 10 == 0:
        print(f"Epoch {epoch+1}/{n_epochs}, Loss: {avg_loss:.4f}")
print("Training finished.")


# Sampling from the Trained Model
def sample(model, num_samples):
    with torch.no_grad():
        # Sample from the base distribution (standard normal)
        z = torch.randn(num_samples, model.input_dim)
        # Apply the inverse transformation to get samples in the original data space
        x_generated, _ = model.inverse(z)
    return x_generated.numpy()

print("\nGenerating new samples from the model...")
generated_samples = sample(model, n_samples)
print(f"Generated samples shape: {generated_samples.shape}")


# Visualization
# Use the same PCA model to transform both original and generated data
pca = PCA(n_components=2)
X_2d = pca.fit_transform(X_np)
generated_2d = pca.transform(generated_samples)

# Create side-by-side plots for comparison
fig, axes = plt.subplots(1, 2, figsize=(16, 7))
fig.suptitle('Comparing Original and Generated Data Distributions (PCA)', fontsize=16)

# Original Data
axes[0].scatter(X_2d[:, 0], X_2d[:, 1], c=y_np, cmap='viridis', alpha=0.7, s=30)
axes[0].set_title('Original Data')
axes[0].set_xlabel('Principal Component 1')
axes[0].set_ylabel('Principal Component 2')
axes[0].grid(True, linestyle='--', alpha=0.6)

# Generated Data
axes[1].scatter(generated_2d[:, 0], generated_2d[:, 1], c='crimson', alpha=0.7, s=30)
axes[1].set_title('Data Generated by Normalizing Flow')
axes[1].set_xlabel('Principal Component 1')
axes[1].set_ylabel('Principal Component 2')
axes[1].grid(True, linestyle='--', alpha=0.6)

plt.show()
print(f"Generated dataset shape: {X.shape}")



